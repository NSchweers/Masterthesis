* The implementation of LISP
  :PROPERTIES:
  :link:     http://www-formal.stanford.edu/jmc/history/lisp/node3.html
  :END:

In the Fall of 1958, I became Assistant Professor of Communication Sciences (in
the EE Department) at M.I.T., and Marvin Minsky (then an assistant professor in
the Mathematics Department) and I started the M.I.T. Artificial Intelligence
Project. The Project was supported by the M.I.T. Research Laboratory of
Electronics which had a contract from the armed services that permitted great
freedom to the Director, Professor Jerome Wiesner, in initiating new projects
that seemed to him of scientific interest. No written proposal was ever
made. When Wiesner asked Minsky and me what we needed for the project, we asked
for a room, two programmers, a secretary and a keypunch, and he asked us to also
undertake the supervision of some of the six mathematics graduate students that
R.L.E. had undertaken to support.

The implementation of LISP began in Fall 1958. The original idea was to produce
a compiler, but this was considered a major undertaking, and we needed some
experimenting in order to get good conventions for subroutine linking, stack
handling and erasure. Therefore, we started by hand-compiling various functions
into assembly language and writing subroutines to provide a LISP
"environment". These included programs to read and print list structure. I can't
now remember whether the decision to use parenthesized list notation as the
external form of LISP data was made then or whether it had already been used in
discussing the paper differentiation program.

The programs to be hand-compiled were written in an informal notation called
M-expressions intended to resemble FORTRAN as much as possible. Besides
FORTRAN-like assignment statements and go tos, the language allowed conditional
expressions and the basic functions of LISP. Allowing recursive function
definitions required no new notation from the function definitions allowed in
FORTRAN I - only the removal of the restriction - as I recall, unstated in the
FORTRAN manual - forbidding recursive definitions. The M-notation also used
brackets instead of parentheses to enclose the arguments of functions in order
to reserve parentheses for list-structure constants. It was intended to compile
from some approximation to the M-notation, but the M-notation was never fully
defined, because representing LISP functions by LISP lists became the dominant
programming language when the interpreter later became available. A machine
readable M-notation would have required redefinition, because the
pencil-and-paper M-notation used characters unavailable on the IBM 026 key
punch.

The READ and PRINT programs induced a de facto standard external notation for
symbolic information, e.g. representing x + 3y + z by (PLUS X (TIMES 3 Y) Z) and
by (ALL (X) (OR (P X) (Q X Y))). Any other notation necessarily requires special
programming, because standard mathematical notations treat different operators
in syntactically different ways. This notation later came to be called
``Cambridge Polish'', because it resembled the prefix notation of Lukasiewicz,
and because we noticed that Quine had also used a parenthesized prefix notation.

The erasure problem also had to be considered, and it was clearly unaesthetic to
use explicit erasure as did IPL. There were two alternatives. The first was to
erase the old contents of a program variable whenever it was updated. Since the
car and cdr operations were not to copy structure, merging list structure would
occur, and erasure would require a system of reference counts. Since there were
only six bits left in a word, and these were in separated parts of the word,
reference counts seemed infeasible without a drastic change in the way list
structures were represented. (A list handling scheme using reference counts was
later used by Collins (1960) on a 48 bit CDC computer).

The second alternative is garbage collection in which storage is abandoned until
the free storage list is exhausted, the storage accessible from program
variables and the stack is marked, and the unmarked storage is made into a new
free storage list. Once we decided on garbage collection, its actual
implementation could be postponed, because only toy examples were being done.

At that time it was also decided to use SAVE and UNSAVE routines that use a
single contiguous public stack array to save the values of variables and
subroutine return addresses in the implementation of recursive subroutines. IPL
built stacks as list structure and their use had to be explicitly
programmed. Another decision was to give up the prefix and tag parts of the
word, to abandon cwr, and to make cons a function of two arguments. This left us
with only a single type - the 15 bit address - so that the language didn't
require declarations.

These simplifications made LISP into a way of describing computable functions
much neater than the Turing machines or the general recursive definitions used
in recursive function theory. The fact that Turing machines constitute an
awkward programming language doesn't much bother recursive function theorists,
because they almost never have any reason to write particular recursive
definitions, since the theory concerns recursive functions in general. They
often have reason to prove that recursive functions with specific properties
exist, but this can be done by an informal argument without having to write them
down explicitly. In the early days of computing, some people developed
programming languages based on Turing machines; perhaps it seemed more
scientific. Anyway, I decided to write a paper describing LISP both as a
programming language and as a formalism for doing recursive function theory. The
paper was Recursive functions of symbolic expressions and their computation by
machine, part I (McCarthy 1960). Part II was never written but was intended to
contain applications to computing with algebraic expressions. The paper had no
influence on recursive function theorists, because it didn't address the
questions that interested them.

One mathematical consideration that influenced LISP was to express programs as
applicative expressions built up from variables and constants using functions. I
considered it important to make these expressions obey the usual mathematical
laws allowing replacement of expressions by expressions giving the same
value. The motive was to allow proofs of properties of programs using ordinary
mathematical methods. This is only possible to the extent that side-effects can
be avoided. Unfortunately, side-effects are often a great convenience when
computational efficiency is important, and ``functions'' with side-effects are
present in LISP. However, the so-called pure LISP is free of side-effects, and
(Cartwright 1976) and (Cartwright and McCarthy 1978) show how to represent pure
LISP programs by sentences and schemata in first order logic and prove their
properties. This is an additional vindication of the striving for mathematical
neatness, because it is now easier to prove that pure LISP programs meet their
specifications than it is for any other programming language in extensive
use. (Fans of other programming languages are challenged to write a program to
concatenate lists and prove that the operation is associative).

Another way to show that LISP was neater than Turing machines was to write a
universal LISP function and show that it is briefer and more comprehensible than
the description of a universal Turing machine. This was the LISP function
eval[e,a], which computes the value of a LISP expression e - the second argument
a being a list of assignments of values to variables. (a is needed to make the
recursion work). Writing eval required inventing a notation representing LISP
functions as LISP data, and such a notation was devised for the purposes of the
paper with no thought that it would be used to express LISP programs in
practice. Logical completeness required that the notation used to express
functions used as functional arguments be extended to provide for recursive
functions, and the LABEL notation was invented by Nathaniel Rochester for that
purpose. D.M.R. Park pointed out that LABEL was logically unnecessary since the
result could be achieved using only LAMBDA - by a construction analogous to
Church's Y-operator, albeit in a more complicated way.

S.R. Russell noticed that eval could serve as an interpreter for LISP, promptly
hand coded it, and we now had a programming language with an interpreter.

The unexpected appearance of an interpreter tended to freeze the form of the
language, and some of the decisions made rather lightheartedly for the
``Recursive functions ...'' paper later proved unfortunate. These included the
COND notation for conditional expressions which leads to an unnecessary depth of
parentheses, and the use of the number zero to denote the empty list NIL and the
truth value false. Besides encouraging pornographic programming, giving a
special interpretation to the address 0 has caused difficulties in all
subsequent implementations.

Another reason for the initial acceptance of awkwardnesses in the internal form
of LISP is that we still expected to switch to writing programs as
M-expressions. The project of defining M-expressions precisely and compiling
them or at least translating them into S-expressions was neither finalized nor
explicitly abandoned. It just receded into the indefinite future, and a new
generation of programmers appeared who preferred internal notation to any
FORTRAN-like or ALGOL-like notation that could be devised.
* From LISP 1 to LISP 1.5
  :PROPERTIES:
  :link:     http://www-formal.stanford.edu/jmc/history/lisp/node4.html
  :END:

** Property lists.
The idea of providing each atom with a list of properties was present in the
first assembly language implementation. It was also one of the theoretical ideas
of the Advice Taker, although the Advice Taker (McCarthy 1959) would have
required a property list for any expression about which information was known
that did not follow from its structure. The READ and PRINT programs required
that the print names of atoms be accessible, and as soon as function definition
became possible, it was necessary to indicate whether a function was a SUBR in
machine code or was an EXPR represented by list structure. Several functions
dealing with property lists were also made available for application programs
which made heavy use of them.

** Insertion of elements in lists and their deletion.
One of the original advertised virtues of list processing for AI work was the
ability to insert and delete elements of lists. Unfortunately, this facility
coexists uneasily with shared list structure. Moreover, operations that insert
and delete don't have a neat representation as functions. LISP has them in the
form of the rplaca and rplacd pseudo-functions, but programs that use them
cannot be conveniently represented in logic, because, regarded as functions,
they don't permit replacement of equals by equals.

** Numbers.
Many computations require both numbers and symbolic expressions. Numbers were
originally implemented in LISP I as lists of atoms, and this proved too slow for
all but the simplest computations. A reasonably efficient implementation of
numbers as atoms in S-expressions was made in LISP 1.5, but in all the early
LISPs, numerical computations were still 10 to 100 times slower than in
FORTRAN. Efficient numerical computation requires some form of typing in the
source language and a distinction between numbers treated by themselves and as
elements of S-expressions. Some recent versions of LISP allow distinguishing
types, but at the time, this seemed incompatible with other features.

** Free variables.
In all innocence, James R. Slagle programmed the following LISP function
definition and complained when it didn't work right:

The object of the function is to find a subexpression of x satisfying p[x] and
return f[x]. If the search is unsuccessful, then the continuation function u[]
of no arguments is to be computed and its value returned. The difficulty was
that when an inner recursion occurred, the value of car[x] wanted was the outer
value, but the inner value was actually used. In modern terminology, lexical
scoping was wanted, and dynamic scoping was obtained.

I must confess that I regarded this difficulty as just a bug and expressed
confidence that Steve Russell would soon fix it. He did fix it but by inventing
the so-called FUNARG device that took the lexical environment along with the
functional argument. Similar difficulties later showed up in Algol 60, and
Russell's turned out to be one of the more comprehensive solutions to the
problem. While it worked well in the interpreter, comprehensiveness and speed
seem to be opposed in compiled code, and this led to a succession of
compromises. Unfortunately, time did not permit writing an appendix giving the
history of the problem, and the interested reader is referred to (Moses 1970) as
a place to start. (David Park tells me that Patrick Fischer also had a hand in
developing the FUNARG device).

** The ``program feature''.
Besides composition of functions and conditional expressions, LISP also allows
sequential programs written with assignment statements and go tos. Compared to
the mathematically elegant recursive function definition features, the ``program
feature'' looks like a hasty afterthought. This is not quite correct; the idea
of having sequential programs in LISP antedates that of having recursive
function definition. However, the notation LISP uses for PROGs was definitely an
afterthought and is far from optimal.

** Once the eval interpreter was programmed,
it became available to the programmer, and it was especially easy to use because
it interprets LISP programs expressed as LISP data. In particular, eval made
possible FEXPRS and FSUBRS which are "functions" that are not given their actual
arguments but are given the expressions that evaluate to the arguments and must
call eval themselves when they want the expressions evaluated. The main
application of this facility is to functions that don't always evaluate all of
their arguments; they evaluate some of them first, and then decide which others
to evaluate. This facility resembles Algol's call-by-name but is more flexible,
because eval is explicitly available. A first order logic treatment of
``extensional'' FEXPRs and FSUBRs now seems possible.

** Since LISP works with lists,
it was also convenient to provide for functions with variable numbers of
arguments by supplying them with a list of arguments rather than the separate
arguments.

Unfortunately, none of the above features has been given a comprehensive and
clear mathematical semantics in connection with LISP or any other programming
language. The best attempt in connection with LISP is Michael Gordon's (1973),
but it is too complicated.

** The first attempt at a compiler
was made by Robert Brayton, but was unsuccessful. The first successful LISP
compiler was programmed by Timothy Hart and Michael Levin. It was written in
LISP and was claimed to be the first compiler written in the language to be
compiled.

Many people participated in the initial development of LISP, and I haven't been
able to remember all their contributions and must settle, at this writing, for a
list of names. I can remember Paul Abrahams, Robert Brayton, Daniel Edwards,
Patrick Fischer, Phyllis Fox, Saul Goldberg, Timothy Hart, Louis Hodes, Michael
Levin, David Luckham, Klim Maling, Marvin Minsky, David Park, Nathaniel
Rochester of IBM, and Steve Russell.

Besides work on the LISP system, many applications were programmed, and this
experience affected the system itself. The main applications that I can remember
are a program by Rochester and Goldberg on symbolic computation of impedances
and other functions associated with electrical networks, J.R. Slagle's thesis
work on symbolic integration directed by Minsky, and Paul Abrahams' thesis on
proof-checking.
